{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare dataset and annotations\n",
    "# Assume you have prepared your dataset with annotations as bounding box coordinates and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"Black Rot\", \"ESCA\", \"Healthy\", \"Leaf Blight\"]\n",
    "num_classes = len(class_names)\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load a pre-trained model\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add custom classification and regression heads\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "class_output = Dense(num_classes, activation=\"softmax\", name=\"class_output\")(x)\n",
    "bbox_output = Dense(num_classes * 4, activation=\"linear\", name=\"bbox_output\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=[class_output, bbox_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"path_to_train_data\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    \"path_to_train_data\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile and train the model\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=learning_rate),\n",
    "    loss=[\"categorical_crossentropy\", \"mean_squared_error\"],\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model\n",
    "evaluation = model.evaluate(validation_generator, steps=validation_steps_per_epoch)\n",
    "print(\"Validation Loss:\", evaluation[0])\n",
    "print(\"Validation Classification Accuracy:\", evaluation[3])\n",
    "print(\"Validation Regression Mean Squared Error:\", evaluation[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Inference\n",
    "# Assuming you have a new leaf image for inference\n",
    "inference_image = load_and_preprocess_image(\n",
    "    \"path_to_inference_image.jpg\"\n",
    ")  # Load and preprocess the image\n",
    "\n",
    "class_predictions, bbox_predictions = model.predict(\n",
    "    tf.expand_dims(inference_image, axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Post-processing\n",
    "# Example of non-maximum suppression for bounding boxes\n",
    "def non_max_suppression(boxes, scores, threshold=0.5):\n",
    "    selected_indices = tf.image.non_max_suppression(\n",
    "        boxes, scores, max_output_size=10, iou_threshold=threshold\n",
    "    )\n",
    "    selected_boxes = tf.gather(boxes, selected_indices)\n",
    "    selected_scores = tf.gather(scores, selected_indices)\n",
    "    return selected_boxes, selected_scores\n",
    "\n",
    "\n",
    "selected_boxes, selected_scores = non_max_suppression(\n",
    "    bbox_predictions[0], class_predictions[0]\n",
    ")\n",
    "\n",
    "# Now you can visualize the selected boxes on the inference image\n",
    "visualize_inference(\n",
    "    image_path=\"path_to_inference_image.jpg\",\n",
    "    boxes=selected_boxes,\n",
    "    scores=selected_scores,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
